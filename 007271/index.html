<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><title>Learning Syntactic Structures from String Input - lingbuzz/007271</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8"/><meta http-equiv="Content-Script-Type" content="text/javascript"/><link rel="canonical" href="/lingbuzz/007271"/><meta name="description" content="This chapter addresses a series of interrelated questions about the origin of syntactic structures: How do language learners generalize from the linguistic stimulus with which they are presented? To w - lingbuzz, the linguistics archive"/><link rel="stylesheet" type="text/css" href="/buzzdocs/styles/article-editor.css"/><link rel="stylesheet" type="text/css" href="/lingbuzz"/></head><body alink="#111111" vlink="#333344" link="#3333AA" onload="onLoad()">&nbsp;<p></p><center><font size="+1"><b><a href="/lingbuzz/007271/current.pdf">Learning Syntactic Structures from String Input</a></b></font><br/><a href="/lingbuzz/007271">Ethan Wilcox</a>, <a href="/lingbuzz/007271">Jon Gauthier</a>, <a href="/lingbuzz/007271">Jennifer Hu</a>, <a href="/lingbuzz/007271">Peng Qian</a>, <a href="/lingbuzz/007271">Roger Levy</a><br/>February 2022</center>&nbsp;<p></p>This chapter addresses a series of interrelated questions about the origin of syntactic structures: How do language learners generalize from the linguistic stimulus with which they are presented? To what extent does linguistic cognition recruit domain-general (i.e. not language-specific) processes and representations? And to what extent are rules and generalizations about linguistic structure separate from rules and generalizations about linguistic meaning? We address these questions by asking what syntactic generalizations can be acquired by a domain-general learner from string input alone. The learning algorithm we deploy is a neural-network based Language Model (GPT-2; Radford et al., 2019), which has been trained to provide probability distributions over strings of text. We assess its linguistic capabilities by treating it like a human subject in a psycholinguistics experiment, and inspect behavior in controlled, factorized tests that are designed to reveal the learning outcomes for one particular syntactic generalization. The tests presented in this chapter focus on a variety of syntactic phenomena in two broad categories: rules about the structure of the sentence and rules about the relationships between smaller lexical units, including scope and binding. Results indicate that our target model has learned many subtle syntactic generalizations, yet it still falls short of humanlike grammatical competence in some areas, notably for cases of parasitic gaps (e.g. “I know what you burned ___ after reading ___ yesterday”). We discuss the implications of these results under three interpretive frameworks, which view the model as (a) a counter-argument against claims of linguistic innateness, (b) a positive example of syntactic emergentism, and (c) a fully-articulated model of grammatical competence.
<table cellspacing="15" valign="top"><tr><td>Format: </td><td>[ <a href="/lingbuzz/007271/current.pdf">pdf</a> ]</td></tr><tr><td>Reference: </td><td>lingbuzz/007271<br/><font size="-1"> (please use that when you cite this article)</font></td></tr><tr><td>Published in: </td><td>Algebraic Structures in Natural Language</td></tr><tr><td>keywords: </td><td>syntax, learnability, deep learning, psycholinguistics, syntax</td></tr><tr><td>Downloaded:</td><td>208 times</td></tr></table><table cellspacing="15"><tr><p></p>&nbsp;<p></p>[ <a href="/lingbuzz/007271">edit this article</a> | <a href="/lingbuzz/007271">back to article list</a> ]</tr></table><script type="text/javascript">/*<![CDATA[*/function onLoad(){};/*]]>*/</script></body></html>