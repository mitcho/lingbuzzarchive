<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><title>No Free Lunch in Linguistics or Machine Learning - lingbuzz/004251</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8"/><meta http-equiv="Content-Script-Type" content="text/javascript"/><link rel="canonical" href="/lingbuzz/004251"/><meta name="description" content="Pater’s target article proposes that neural networks will provide theories of learning that generative grammar lacks. We argue that his enthusiasm is premature since the biases of neural networks are  - lingbuzz, the linguistics archive"/><link rel="stylesheet" type="text/css" href="/buzzdocs/styles/article-editor.css"/><link rel="stylesheet" type="text/css" href="/lingbuzz"/></head><body alink="#111111" vlink="#333344" link="#3333AA" onload="onLoad()">&nbsp;<p></p><center><font size="+1"><b><a href="/lingbuzz/004251/current.pdf">No Free Lunch in Linguistics or Machine Learning</a></b></font><br/><a href="/lingbuzz/004251">Jonathan Rawski</a>, <a href="/lingbuzz/004251">Jeffrey Nicholas Heinz</a><br/>January 2019</center>&nbsp;<p></p>Pater’s target article proposes that neural networks will provide theories of learning that generative grammar lacks. We argue that his enthusiasm is premature since the biases of neural networks are largely unknown, and he disregards decades of work on machine learning and learnability. Learning biases form a two-way street: all learners have biases, and those biases constrain the space of learnable grammars in mathematically measurable ways. Analytical methods from the related fields of computational learning theory and grammatical inference allow one to study language learning, neural networks, and linguistics at an appropriate level of abstraction. The only way to satisfy our hunger and to make progress on the science of language learning is to confront these core issues directly.<table cellspacing="15" valign="top"><tr><td>Format: </td><td>[ <a href="/lingbuzz/004251/current.pdf">pdf</a> ]</td></tr><tr><td>Reference: </td><td>lingbuzz/004251<br/><font size="-1"> (please use that when you cite this article)</font></td></tr><tr><td>Published in: </td><td>Language, https://muse.jhu.edu/article/718443/pdf</td></tr><tr><td>keywords: </td><td>learnability, computational learning theory, neural networks, poverty of the stimulus, language acquisition, syntax, phonology, semantics, morphology</td></tr><tr><td>previous versions: </td><td><a href="/lingbuzz/004251/v2.pdf">v2 [November 2018]</a><br/><a href="/lingbuzz/004251/v1.pdf">v1 [October 2018]</a><br/></td></tr><tr><td>Downloaded:</td><td>737 times</td></tr></table><table cellspacing="15"><tr><p></p>&nbsp;<p></p>[ <a href="/lingbuzz/004251">edit this article</a> | <a href="/lingbuzz/004251">back to article list</a> ]</tr></table><script type="text/javascript">/*<![CDATA[*/function onLoad(){};/*]]>*/</script></body></html>