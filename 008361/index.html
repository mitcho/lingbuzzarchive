<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><title>Modeling Regularization in Language Acquisition as Noise-Tolerant Grammar Selection - lingbuzz/008361</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8"/><meta http-equiv="Content-Script-Type" content="text/javascript"/><link rel="canonical" href="/lingbuzz/008361"/><meta name="description" content="Language acquisition involves drawing systematic generalizations from messy data. On one hypothesis, this is facilitated by a domain-general bias for children to “regularize” inconsistent variability, - lingbuzz, the linguistics archive"/><link rel="stylesheet" type="text/css" href="/buzzdocs/styles/article-editor.css"/><link rel="stylesheet" type="text/css" href="/lingbuzz"/></head><body alink="#111111" vlink="#333344" link="#3333AA" onload="onLoad()">&nbsp;<p></p><center><font size="+1"><b><a href="/lingbuzz/008361/current.pdf">Modeling Regularization in Language Acquisition as Noise-Tolerant Grammar Selection</a></b></font><br/><a href="/lingbuzz/008361">Laurel Perkins</a>, <a href="/lingbuzz/008361">Tim Hunter</a><br/>August 2024</center>&nbsp;<p></p>Language acquisition involves drawing systematic generalizations from messy data. On one hypothesis, this is facilitated by a domain-general bias for children to “regularize” inconsistent variability, sharpening the statistical distributions in their input towards more systematic extremes. We introduce a general computational framework for modeling a different explanation: on this view, children expect that their data are a noisy realization of a restrictive underlying grammatical system. We implement a learner that evaluates a choice among composite context-free grammars, in which a restricted set of “core” rules, comprising the particular grammatical processes that the learner is currently trying to acquire, operate alongside a less restricted set of “noise” rules, representing other independent processes that have yet to be learned, and conspire to introduce variability into the data. Our “Noisy CFG Learner” partitions its data into portions that serve as evidence for one of the possible core grammars in its hypothesis space, and portions generated by these noise processes. It does so without knowing in advance how much noise occurs or what its properties are. 

We compare our learner to a common implementation of the general regularization bias approach, and
show that both can account for children’s behavior in a representative artificial language learning experiment. However, we find that our approach performs better on two naturalistic case studies in early syntax acquisition: learning the rules governing canonical word-order and case-marking, given natural language data with “noise” from non-canonical sentence types. We show that our learner succeeds because its architecture allows a natural way to express linguistically-motivated expectations about the character of those rules. This suggests that, in certain domains, successful learning from messy data may be enabled by a hypothesis space comprising restrictive grammatical options.<table cellspacing="15" valign="top"><tr><td>Format: </td><td>[ <a href="/lingbuzz/008361/current.pdf">pdf</a> ]</td></tr><tr><td>Reference: </td><td>lingbuzz/008361<br/><font size="-1"> (please use that when you cite this article)</font></td></tr><tr><td>Published in: </td><td>under review</td></tr><tr><td>keywords: </td><td>language acquisition, grammar, regularization, computational modelling, bayesian reasoning, morphology, syntax</td></tr><tr><td>Downloaded:</td><td>302 times</td></tr></table><table cellspacing="15"><tr><p></p>&nbsp;<p></p>[ <a href="/lingbuzz/008361">edit this article</a> | <a href="/lingbuzz/008361">back to article list</a> ]</tr></table><script type="text/javascript">/*<![CDATA[*/function onLoad(){};/*]]>*/</script></body></html>