<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><title>Can neural networks acquire a structural bias from raw linguistic data? - lingbuzz/005312</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8"/><meta http-equiv="Content-Script-Type" content="text/javascript"/><link rel="canonical" href="/lingbuzz/005312"/><meta name="description" content="We evaluate whether BERT, a widely used neural network for sentence processing, acquires an inductive bias towards forming structural generalizations through pretraining on raw data. We conduct four e - lingbuzz, the linguistics archive"/><link rel="stylesheet" type="text/css" href="/buzzdocs/styles/article-editor.css"/><link rel="stylesheet" type="text/css" href="/lingbuzz"/></head><body alink="#111111" vlink="#333344" link="#3333AA" onload="onLoad()">&nbsp;<p></p><center><font size="+1"><b><a href="/lingbuzz/005312/current.pdf">Can neural networks acquire a structural bias from raw linguistic data?</a></b></font><br/><a href="/lingbuzz/005312">Alex Warstadt</a>, <a href="/lingbuzz/005312">Samuel Bowman</a><br/>May 2020</center>&nbsp;<p></p>We evaluate whether BERT, a widely used neural network for sentence processing, acquires an inductive bias towards forming structural generalizations through pretraining on raw data. We conduct four experiments testing its preference for structural vs. linear generalizations in different structure-dependent phenomena. We find that BERT makes a structural generalization in 3 out of 4 empirical domains---subject-auxiliary inversion, reflexive binding, and verb tense detection in embedded clauses---but makes a linear generalization when tested on NPI licensing. We argue that these results are the strongest evidence so far from artificial learners supporting the proposition that a structural bias can be acquired from raw data. If this conclusion is correct, it is tentative evidence that some linguistic universals can be acquired by learners without innate biases. However, the precise implications for human language acquisition are unclear, as humans learn language from significantly less data than BERT.<table cellspacing="15" valign="top"><tr><td>Format: </td><td>[ <a href="/lingbuzz/005312/current.pdf">pdf</a> ]</td></tr><tr><td>Reference: </td><td>lingbuzz/005312<br/><font size="-1"> (please use that when you cite this article)</font></td></tr><tr><td>Published in: </td><td>To appear in Proceedings of 42nd Annual Virtual Meeting of the Cognitive Science Society</td></tr><tr><td>keywords: </td><td>inductive bias, structure dependence, bert, learnability of grammar, poverty of the stimulus, neural network, self-supervised learning, syntax</td></tr><tr><td>Downloaded:</td><td>422 times</td></tr></table><table cellspacing="15"><tr><p></p>&nbsp;<p></p>[ <a href="/lingbuzz/005312">edit this article</a> | <a href="/lingbuzz/005312">back to article list</a> ]</tr></table><script type="text/javascript">/*<![CDATA[*/function onLoad(){};/*]]>*/</script></body></html>