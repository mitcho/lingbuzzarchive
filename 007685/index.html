<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><title>Can Language Models Be Tricked by Language Illusions? Easier with Syntax, Harder with Semantics - lingbuzz/007685</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8"/><meta http-equiv="Content-Script-Type" content="text/javascript"/><link rel="canonical" href="/lingbuzz/007685"/><meta name="description" content="Language models (LMs) have been argued to overlap substantially with human beings in grammaticality judgment tasks. But when humans systematically make errors in language processing, should we expect  - lingbuzz, the linguistics archive"/><link rel="stylesheet" type="text/css" href="/buzzdocs/styles/article-editor.css"/><link rel="stylesheet" type="text/css" href="/lingbuzz"/></head><body alink="#111111" vlink="#333344" link="#3333AA" onload="onLoad()">&nbsp;<p></p><center><font size="+1"><b><a href="/lingbuzz/007685/current.pdf">Can Language Models Be Tricked by Language Illusions? Easier with Syntax, Harder with Semantics</a></b></font><br/><a href="/lingbuzz/007685">Yuhan Zhang</a>, <a href="/lingbuzz/007685">Edward Gibson</a>, <a href="/lingbuzz/007685">Forrest Davis</a><br/>October 2023</center>&nbsp;<p></p>Language models (LMs) have been argued to overlap substantially with human beings in grammaticality judgment tasks. But when humans systematically make errors in language processing, should we expect LMs to behave like cognitive models of language and mimic human behavior? We answer this question by investigating LMs' more subtle judgments associated with "language illusions" -- sentences that are vague in meaning, implausible, or ungrammatical but receive unexpectedly high acceptability judgments by humans. We looked at three illusions: the comparative illusion (e.g. "More people have been to Russia than I have"), the depth-charge illusion (e.g. "No head injury is too trivial to be ignored"), and the negative polarity item (NPI) illusion (e.g. "The hunter who no villager believed to be trustworthy will ever shoot a bear"). We found that probabilities represented by LMs were more likely to align with human judgments of being "tricked" by the NPI illusion which examines a structural dependency, compared to the comparative and the depth-charge illusions which require sophisticated semantic understanding. No single LM or metric yielded results that are entirely consistent with human behavior. Ultimately, we show that LMs are limited both in their construal as cognitive models of human language processing and in their capacity to recognize nuanced but critical information in complicated language materials.<table cellspacing="15" valign="top"><tr><td>Format: </td><td>[ <a href="/lingbuzz/007685/current.pdf">pdf</a> ]</td></tr><tr><td>Reference: </td><td>lingbuzz/007685<br/><font size="-1"> (please use that when you cite this article)</font></td></tr><tr><td>Published in: </td><td>The SIGNLL Conference on Computational Natural Language Learning (CoNLL) 2023</td></tr><tr><td>keywords: </td><td>natural language understanding, language illusion, sentence processing, semantics, syntax</td></tr><tr><td>Downloaded:</td><td>326 times</td></tr></table><table cellspacing="15"><tr><p></p>&nbsp;<p></p>[ <a href="/lingbuzz/007685">edit this article</a> | <a href="/lingbuzz/007685">back to article list</a> ]</tr></table><script type="text/javascript">/*<![CDATA[*/function onLoad(){};/*]]>*/</script></body></html>