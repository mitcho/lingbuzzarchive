<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><title>Embedding and measurement of vowels using machine perception - lingbuzz/006611</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8"/><meta http-equiv="Content-Script-Type" content="text/javascript"/><link rel="canonical" href="/lingbuzz/006611"/><meta name="description" content="We present a spatial embedding and measurement method for vowel sounds based on the output of a convolutional neural network (CNN) which has been trained to recognize phonemic categories from spectrog - lingbuzz, the linguistics archive"/><link rel="stylesheet" type="text/css" href="/buzzdocs/styles/article-editor.css"/><link rel="stylesheet" type="text/css" href="/lingbuzz"/></head><body alink="#111111" vlink="#333344" link="#3333AA" onload="onLoad()">&nbsp;<p></p><center><font size="+1"><b><a href="/lingbuzz/006611/current.pdf">Embedding and measurement of vowels using machine perception</a></b></font><br/><a href="/lingbuzz/006611">James Burridge</a>, <a href="/lingbuzz/006611">Bert Vaux</a><br/>February 2022</center>&nbsp;<p></p>We present a spatial embedding and measurement method for vowel sounds based on the output of a convolutional neural network (CNN) which has been trained to recognize phonemic categories from spectrograms, and has similar perceptual behaviour to humans. We define the perceptual similarity between two categories as a typical listenerâ€™s degree of belief that one category was intended by a typical talker given that the other was uttered. In low-dimensional "chart-space" we model utterance distributions, conditional on phonemic category, as Gaussian, with means chosen to match the perceptual characteristics of the CNN as closely as possible. In this way, perceptual similarities between high dimensional spectrogram images encoded by the CNN are given a low dimensional representation in the chart. We then present a likelihood-based mapping from acoustic to chart space, via the output of the CNN. This generates low dimensional measurements of individual vowel spectrograms. Our method can in principle be generalized to measure any form of acoustic variation, provided we are able to first define a set of categories of sounds which describe it. The potential value of our approach is that it provides a means of producing perceptually meaningful measurements of sounds directly from their spectrograms. <table cellspacing="15" valign="top"><tr><td>Format: </td><td>[ <a href="/lingbuzz/006611/current.pdf">pdf</a> ]</td></tr><tr><td>Reference: </td><td>lingbuzz/006611<br/><font size="-1"> (please use that when you cite this article)</font></td></tr><tr><td>Published in: </td><td></td></tr><tr><td>keywords: </td><td>vowels, measurement, perceptual, machine perception, phonology</td></tr><tr><td>Downloaded:</td><td>266 times</td></tr></table><table cellspacing="15"><tr><p></p>&nbsp;<p></p>[ <a href="/lingbuzz/006611">edit this article</a> | <a href="/lingbuzz/006611">back to article list</a> ]</tr></table><script type="text/javascript">/*<![CDATA[*/function onLoad(){};/*]]>*/</script></body></html>