<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><title>Fodor and Pylyshyn’s systematicity challenge still stands: A reply to Lake and Baroni (2023) - lingbuzz/007759</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8"/><meta http-equiv="Content-Script-Type" content="text/javascript"/><link rel="canonical" href="/lingbuzz/007759"/><meta name="description" content="The recent successes of neural networks producing human-like language have captured the attention of the general public. They have also caused significant stir in cognitive science, with many research - lingbuzz, the linguistics archive"/><link rel="stylesheet" type="text/css" href="/buzzdocs/styles/article-editor.css"/><link rel="stylesheet" type="text/css" href="/lingbuzz"/></head><body alink="#111111" vlink="#333344" link="#3333AA" onload="onLoad()">&nbsp;<p></p><center><font size="+1"><b><a href="/lingbuzz/007759/current.pdf">Fodor and Pylyshyn’s systematicity challenge still stands: A reply to Lake and Baroni (2023)</a></b></font><br/><a href="/lingbuzz/007759">Michael Goodale</a>, <a href="/lingbuzz/007759">Salvador Mascarenhas</a><br/>December 2023</center>&nbsp;<p></p>The recent successes of neural networks producing human-like language have captured the attention of the general public. They have also caused significant stir in cognitive science, with many researchers arguing that classical puzzles about human cognition and challenges to artificial intelligence are being solved by neural networks. An article recently published in Nature, covered by the journal’s media department as a “breakthrough” in AI, argues that a particular machine-learning technique has succeeded where others failed: to match and perhaps explain the human ability to reverse engineer generative processes (rules) based on few examples. We demonstrate that these conclusions are premature. Among other results, we found that the model displays different rates of generalization success depending on what labels are attached to what meanings. This is in sharp contrast with the fact that there are no linguistic or broader cognitive benefits from calling a carbonated beverage “pop” or “soda,” nor from calling the objects of study of dendrology “trees” or “Bäume.” Crucially, our examples of failures lie squarely within the narrow task that the article focuses on, calling into question the ambitious conclusions and the bullish media coverage the article received.<table cellspacing="15" valign="top"><tr><td>Format: </td><td>[ <a href="/lingbuzz/007759/current.pdf">pdf</a> ]</td></tr><tr><td>Reference: </td><td>lingbuzz/007759<br/><font size="-1"> (please use that when you cite this article)</font></td></tr><tr><td>Published in: </td><td>submitted to Nature: Matters Arising</td></tr><tr><td>keywords: </td><td>neural networks, systematicity, compositionality, semantics, syntax</td></tr><tr><td>Downloaded:</td><td>408 times</td></tr></table><table cellspacing="15"><tr><p></p>&nbsp;<p></p>[ <a href="/lingbuzz/007759">edit this article</a> | <a href="/lingbuzz/007759">back to article list</a> ]</tr></table><script type="text/javascript">/*<![CDATA[*/function onLoad(){};/*]]>*/</script></body></html>