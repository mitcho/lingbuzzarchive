<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><title>Identity-Based Patterns in Deep Convolutional Networks: Generative Adversarial Phonology and  Reduplication - lingbuzz/005438</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8"/><meta http-equiv="Content-Script-Type" content="text/javascript"/><link rel="canonical" href="/lingbuzz/005438"/><meta name="description" content="This paper models unsupervised learning of an identity-based pattern (or copying) in speech called reduplication from raw continuous data with deep convolutional neural networks. We use the ciwGAN arc - lingbuzz, the linguistics archive"/><link rel="stylesheet" type="text/css" href="/buzzdocs/styles/article-editor.css"/><link rel="stylesheet" type="text/css" href="/lingbuzz"/></head><body alink="#111111" vlink="#333344" link="#3333AA" onload="onLoad()">&nbsp;<p></p><center><font size="+1"><b><a href="/lingbuzz/005438/current.pdf">Identity-Based Patterns in Deep Convolutional Networks: Generative Adversarial Phonology and  Reduplication</a></b></font><br/><a href="/lingbuzz/005438">Gasper Begus</a><br/>November 2021</center>&nbsp;<p></p>This paper models unsupervised learning of an identity-based pattern (or copying) in speech called reduplication from raw continuous data with deep convolutional neural networks. We use the ciwGAN architecture Beguš (2021a) in which learning of meaningful representations in speech emerges from a requirement that the CNNs generate informative data. We propose a technique to wug-test CNNs trained on speech and, based on four generative tests, argue that the network learns to represent an identity-based pattern in its latent space. By manipulating only two categorical variables in the latent space, we can actively turn an unreduplicated form into a reduplicated form with no other substantial changes to the output in the majority of cases. We also argue that the network extends the identity-based pattern to unobserved data. Exploration of how meaningful representations of identity-based patterns emerge in CNNs and how the latent space variables outside of the training range correlate with identity-based patterns in the output has general implications for neural network interpretability.<table cellspacing="15" valign="top"><tr><td>Format: </td><td>[ <a href="/lingbuzz/005438/current.pdf">pdf</a> ]</td></tr><tr><td>Reference: </td><td>lingbuzz/005438<br/><font size="-1"> (please use that when you cite this article)</font></td></tr><tr><td>Published in: </td><td>Transactions of the Association for Computational Linguistics (TACL) 2021. 9: 1180–1196. https://doi.org/10.1162/tacl_a_00421</td></tr><tr><td>keywords: </td><td>artificial intelligence, generative adversarial networks, speech, identity-based learning, neural network interpretability, reduplication, phonology</td></tr><tr><td>previous versions: </td><td><a href="/lingbuzz/005438/v2.pdf">v2 [July 2021]</a><br/><a href="/lingbuzz/005438/v1.pdf">v1 [September 2020]</a><br/></td></tr><tr><td>Downloaded:</td><td>681 times</td></tr></table><table cellspacing="15"><tr><p></p>&nbsp;<p></p>[ <a href="/lingbuzz/005438">edit this article</a> | <a href="/lingbuzz/005438">back to article list</a> ]</tr></table><script type="text/javascript">/*<![CDATA[*/function onLoad(){};/*]]>*/</script></body></html>