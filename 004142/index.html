<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><title>Fusion is great, and interpretable fusion could be exciting for theory generation - lingbuzz/004142</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8"/><meta http-equiv="Content-Script-Type" content="text/javascript"/><link rel="canonical" href="/lingbuzz/004142"/><meta name="description" content="Response to “Generative linguistics and neural networks at 60: foundation, friction, and fusion” by Joe Pater.

From my perspective, Pater’s (2018) target article does a great service to both resear - lingbuzz, the linguistics archive"/><link rel="stylesheet" type="text/css" href="/buzzdocs/styles/article-editor.css"/><link rel="stylesheet" type="text/css" href="/lingbuzz"/></head><body alink="#111111" vlink="#333344" link="#3333AA" onload="onLoad()">&nbsp;<p></p><center><font size="+1"><b><a href="/lingbuzz/004142/current.pdf">Fusion is great, and interpretable fusion could be exciting for theory generation</a></b></font><br/><a href="/lingbuzz/004142">Lisa Pearl</a><br/>September 2018</center>&nbsp;<p></p>Response to “Generative linguistics and neural networks at 60: foundation, friction, and fusion” by Joe Pater.

From my perspective, Pater’s (2018) target article does a great service to both researchers who work in generative linguistics and researchers who utilize neural networks – and especially to researchers who might find themselves wanting to do both by harnessing the insights of each tradition. The fusion of theories of linguistic representation and probabilistic learning techniques has certainly led to many interesting and valuable insights about the nature of both linguistic representation and the language acquisition process. However, I feel that the most exciting aspect of Pater’s article is the increasing interpretability of neural network models, especially when combined with insights from the generative linguistics theoretical framework. This allows for the possibility that neural networks could be used to actually generate new theories of representation. I describe how I think this theory generation process might work with interpretable neural networks.<table cellspacing="15" valign="top"><tr><td>Format: </td><td>[ <a href="/lingbuzz/004142/current.pdf">pdf</a> ]</td></tr><tr><td>Reference: </td><td>lingbuzz/004142<br/><font size="-1"> (please use that when you cite this article)</font></td></tr><tr><td>Published in: </td><td>(submitted to Perspectives subsection of Language)</td></tr><tr><td>keywords: </td><td>generative linguistics, neural networks, probabilistic learning, language acquisition, theory generation, bayesian inference, learnability, syntax, phonology, semantics, morphology</td></tr><tr><td>previous versions: </td><td><a href="/lingbuzz/004142/v1.pdf">v1 [July 2018]</a><br/></td></tr><tr><td>Downloaded:</td><td>220 times</td></tr></table><table cellspacing="15"><tr><p></p>&nbsp;<p></p>[ <a href="/lingbuzz/004142">edit this article</a> | <a href="/lingbuzz/004142">back to article list</a> ]</tr></table><script type="text/javascript">/*<![CDATA[*/function onLoad(){};/*]]>*/</script></body></html>
