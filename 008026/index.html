<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><title>The simple reason LLMs are not scientific models (and what the alternative is for linguistics). - lingbuzz/008026</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8"/><meta http-equiv="Content-Script-Type" content="text/javascript"/><link rel="canonical" href="/lingbuzz/008026"/><meta name="description" content="Response to Piantadosi (2023). There is a an explicit and mathematical reason why Large Language Models (LLMs) are not scientific theories. They belong to a class of Universal Function Approximators,  - lingbuzz, the linguistics archive"/><link rel="stylesheet" type="text/css" href="/buzzdocs/styles/article-editor.css"/><link rel="stylesheet" type="text/css" href="/lingbuzz"/></head><body alink="#111111" vlink="#333344" link="#3333AA" onload="onLoad()">&nbsp;<p></p><center><font size="+1"><b><a href="/lingbuzz/008026/current.pdf">The simple reason LLMs are not scientific models (and what the alternative is for linguistics).</a></b></font><br/><a href="/lingbuzz/008026">Joe Collins</a><br/>April 2024</center>&nbsp;<p></p>Response to Piantadosi (2023). There is a an explicit and mathematical reason why Large Language Models (LLMs) are not scientific theories. They belong to a class of Universal Function Approximators, which can approximate any mathematical function by summing over many generic functions, and whose representations are therefore arbitrary. They are closely related to approximation methods such as (generalised) Fourier series and Taylor expansions.

This has consequences for how much we can learn from the practical limitations of LLMs and their behaviour.

Finally, it is argued if linguistics is to tackle problems of complexity and emergence, it should take its cues from similarly "Galilean" fields such as statistical physics, rather than machine learning.<table cellspacing="15" valign="top"><tr><td>Format: </td><td>[ <a href="/lingbuzz/008026/current.pdf">pdf</a> ]</td></tr><tr><td>Reference: </td><td>lingbuzz/008026<br/><font size="-1"> (please use that when you cite this article)</font></td></tr><tr><td>Published in: </td><td></td></tr><tr><td>keywords: </td><td>piantadosi 2023, large language models, llms, chatgpt, transformers, ai, generative linguistics, chomsky, physics, syntax, phonology, semantics, morphology, machine learning, deep learning, connectionism</td></tr><tr><td>previous versions: </td><td><a href="/lingbuzz/008026/v1.pdf">v1 [April 2024]</a><br/></td></tr><tr><td>Downloaded:</td><td>686 times</td></tr></table><table cellspacing="15"><tr><p></p>&nbsp;<p></p>[ <a href="/lingbuzz/008026">edit this article</a> | <a href="/lingbuzz/008026">back to article list</a> ]</tr></table><script type="text/javascript">/*<![CDATA[*/function onLoad(){};/*]]>*/</script></body></html>