<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><title>Data convergence in syntactic theory and the role of sentence pairs - lingbuzz/005453</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8"/><meta http-equiv="Content-Script-Type" content="text/javascript"/><link rel="canonical" href="/lingbuzz/005453"/><meta name="description" content="Most acceptability judgments reported in the syntactic literature are obtained by linguists being their own informants. For well-represented languages like English, this method of data collection is b - lingbuzz, the linguistics archive"/><link rel="stylesheet" type="text/css" href="/buzzdocs/styles/article-editor.css"/><link rel="stylesheet" type="text/css" href="/lingbuzz"/></head><body alink="#111111" vlink="#333344" link="#3333AA" onload="onLoad()">&nbsp;<p></p><center><font size="+1"><b><a href="/lingbuzz/005453/current.pdf">Data convergence in syntactic theory and the role of sentence pairs</a></b></font><br/><a href="/lingbuzz/005453">Tom S. Juzek</a>, <a href="/lingbuzz/005453">Jana Häussler</a><br/>January 2020</center>&nbsp;<p></p>Most acceptability judgments reported in the syntactic literature are obtained by linguists being their own informants. For well-represented languages like English, this method of data collection is best described as a process of community agreement, given that linguists typically discuss their judgments with colleagues. However, the process itself is comparably opaque, and the reliability of its output has been questioned. Recent studies looking into this criticism have shown that judgments reported in the literature for English can be replicated in quantitative experiments to a near-perfect degree. However, the focus of those studies has been on testing sentence pairs. We argue that replication of only contrasts is not sufficient, because theory building necessarily includes comparison across pairs and across papers. Thus, we test items at large, i.e. independent of counterparts. We created a corpus of grammaticality judgments on sequences of American English reported in articles published in Linguistic Inquiry and then collected experimental ratings for a random subset of them. Overall, expert ratings and experimental ratings converge to a good degree, but there are numerous instances in which ratings do not converge. Based on this, we argue that for theory-critical data, the process of community agreement should be accompanied by quantitative methods whenever possible.<table cellspacing="15" valign="top"><tr><td>Format: </td><td>[ <a href="/lingbuzz/005453/current.pdf">pdf</a> ]</td></tr><tr><td>Reference: </td><td>lingbuzz/005453<br/><font size="-1"> (please use that when you cite this article)</font></td></tr><tr><td>Published in: </td><td>Zeitschrift für Sprachwissenschaft</td></tr><tr><td>keywords: </td><td>experimental syntax, data convergence, data reliability, linguistic methodology, grammaticality judgments, acceptability judgment tasks, introspection, syntax, grammar theory, syntatic theory</td></tr><tr><td>Downloaded:</td><td>344 times</td></tr></table><table cellspacing="15"><tr><p></p>&nbsp;<p></p>[ <a href="/lingbuzz/005453">edit this article</a> | <a href="/lingbuzz/005453">back to article list</a> ]</tr></table><script type="text/javascript">/*<![CDATA[*/function onLoad(){};/*]]>*/</script></body></html>