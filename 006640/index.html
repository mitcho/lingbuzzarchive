<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><title>Transformers in the loop: Polarity in neural models of language - lingbuzz/006640</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8"/><meta http-equiv="Content-Script-Type" content="text/javascript"/><link rel="canonical" href="/lingbuzz/006640"/><meta name="description" content="Representation of linguistic phenomena in computational language models is typically assessed against the predictions of existing linguistic theories of these phenomena. Using the notion of polarity a - lingbuzz, the linguistics archive"/><link rel="stylesheet" type="text/css" href="/buzzdocs/styles/article-editor.css"/><link rel="stylesheet" type="text/css" href="/lingbuzz"/></head><body alink="#111111" vlink="#333344" link="#3333AA" onload="onLoad()">&nbsp;<p></p><center><font size="+1"><b><a href="/lingbuzz/006640/current.pdf">Transformers in the loop: Polarity in neural models of language</a></b></font><br/><a href="/lingbuzz/006640">Lisa Bylinina</a>, <a href="/lingbuzz/006640">Alexey Tikhonov</a><br/>May 2022</center>&nbsp;<p></p>Representation of linguistic phenomena in computational language models is typically assessed against the predictions of existing linguistic theories of these phenomena. Using the notion of polarity as a case study, we show that this is not always the most adequate set-up. We probe polarity via so-called 'negative polarity items' (in particular, English 'any') in two pre-trained Transformer-based models (BERT and GPT-2). We show that - at least for polarity - metrics derived from language models are more consistent with data from psycholinguistic experiments than linguistic theory predictions. Establishing this allows us to more adequately evaluate the performance of language models and also to use language models to discover new insights into natural language grammar beyond existing linguistic theories. This work contributes to establishing closer ties between psycholinguistic experiments and experiments with language models.<table cellspacing="15" valign="top"><tr><td>Format: </td><td>[ <a href="/lingbuzz/006640/current.pdf">pdf</a> ]</td></tr><tr><td>Reference: </td><td>lingbuzz/006640<br/><font size="-1"> (please use that when you cite this article)</font></td></tr><tr><td>Published in: </td><td>ACL2022 main conference proceedings</td></tr><tr><td>keywords: </td><td>polarity, npis, language models, semantics</td></tr><tr><td>Downloaded:</td><td>98 times</td></tr></table><table cellspacing="15"><tr><p></p>&nbsp;<p></p>[ <a href="/lingbuzz/006640">edit this article</a> | <a href="/lingbuzz/006640">back to article list</a> ]</tr></table><script type="text/javascript">/*<![CDATA[*/function onLoad(){};/*]]>*/</script></body></html>