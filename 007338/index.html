<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><title>Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale - lingbuzz/007338</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8"/><meta http-equiv="Content-Script-Type" content="text/javascript"/><link rel="canonical" href="/lingbuzz/007338"/><meta name="description" content="Large language models (LLMs) have achieved a milestone that undenia-bly changed many held beliefs in artificial intelligence (AI). However, there remains many limitations of these LLMs when it comes t - lingbuzz, the linguistics archive"/><link rel="stylesheet" type="text/css" href="/buzzdocs/styles/article-editor.css"/><link rel="stylesheet" type="text/css" href="/lingbuzz"/></head><body alink="#111111" vlink="#333344" link="#3333AA" onload="onLoad()">&nbsp;<p></p><center><font size="+1"><b><a href="/lingbuzz/007338/current.pdf">Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale</a></b></font><br/><a href="/lingbuzz/007338">Walid Saba</a><br/>July 2023</center>&nbsp;<p></p>Large language models (LLMs) have achieved a milestone that undenia-bly changed many held beliefs in artificial intelligence (AI). However, there remains many limitations of these LLMs when it comes to true language understanding, limitations that are a byproduct of the under-lying architecture of deep neural networks. Moreover, and due to their subsymbolic nature, whatever knowledge these models acquire about how language works will always be buried in billions of microfeatures (weights), none of which is meaningful on its own, making such models hopelessly unexplainable. To address these limitations, we suggest com-bining the strength of symbolic representations with what we believe to be the key to the success of LLMs, namely a successful bottom-up re-verse engineering of language at scale. As such we argue for a bottom-up reverse engineering of language in a symbolic setting. Hints on what this project amounts to have been suggested by several authors, and we discuss in some detail here how this project could be accomplished.<table cellspacing="15" valign="top"><tr><td>Format: </td><td>[ <a href="/lingbuzz/007338/current.pdf">pdf</a> ]</td></tr><tr><td>Reference: </td><td>lingbuzz/007338<br/><font size="-1"> (please use that when you cite this article)</font></td></tr><tr><td>Published in: </td><td></td></tr><tr><td>keywords: </td><td>bottom-up reverse engineering of language, symbolic nlp, semantics</td></tr><tr><td>previous versions: </td><td><a href="/lingbuzz/007338/v6.pdf">v6 [July 2023]</a><br/><a href="/lingbuzz/007338/v5.pdf">v5 [July 2023]</a><br/><a href="/lingbuzz/007338/v4.pdf">v4 [July 2023]</a><br/><a href="/lingbuzz/007338/v3.pdf">v3 [July 2023]</a><br/><a href="/lingbuzz/007338/v2.pdf">v2 [July 2023]</a><br/><a href="/lingbuzz/007338/v1.pdf">v1 [May 2023]</a><br/></td></tr><tr><td>Downloaded:</td><td>736 times</td></tr></table><table cellspacing="15"><tr><p></p>&nbsp;<p></p>[ <a href="/lingbuzz/007338">edit this article</a> | <a href="/lingbuzz/007338">back to article list</a> ]</tr></table><script type="text/javascript">/*<![CDATA[*/function onLoad(){};/*]]>*/</script></body></html>