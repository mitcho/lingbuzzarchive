<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><title>A Note on Artificial Intelligence and the critical recursive implementation: The lagging problem of ‘background knowledge - lingbuzz/006410</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8"/><meta http-equiv="Content-Script-Type" content="text/javascript"/><link rel="canonical" href="/lingbuzz/006410"/><meta name="description" content="Note 4: A Note on Artificial Intelligence and the Critical Recursive Implementation. 

Opening Remarks:
Most historians of the Cognitive Revolution consider the now historic 1956 MIT IRE Conference - lingbuzz, the linguistics archive"/><link rel="stylesheet" type="text/css" href="/buzzdocs/styles/article-editor.css"/><link rel="stylesheet" type="text/css" href="/lingbuzz"/></head><body alink="#111111" vlink="#333344" link="#3333AA" onload="onLoad()">&nbsp;<p></p><center><font size="+1"><b><a href="/lingbuzz/006410/current.pdf">A Note on Artificial Intelligence and the critical recursive implementation: The lagging problem of ‘background knowledge</a></b></font><br/><a href="/lingbuzz/006410">Joseph Galasso</a><br/>June 2019</center>&nbsp;<p></p>Note 4: A Note on Artificial Intelligence and the Critical Recursive Implementation. 

Opening Remarks:
Most historians of the Cognitive Revolution consider the now historic 1956 MIT IRE Conference ‘Transactions on Information Theory’ to be the conceptual origin of the revolution. It was at this conference that three of the most important papers in the emerging field of AI would be read: 
(i)	George Miller’s Human memory and the storage of information (coupled with an earlier 1955 paper The magic number seven, plus or minus two: Some limits on our capacity for processing information).
(ii)	Allen Newell & Herbert Simon’s paper The logic Theory Machine: A complex Information processing system.
(iii)	Noam Chomsky’s paper Three models for the description of language.

But it would not be long before splits would occur in the very defining of AI. For some, let’s call them the AI-soft crowd, despite the ever-growing consensus that the brain really did not function like a computer after all, (as was earlier suggested by the naïve ‘brain is computer’ metaphor of the time), the AI-soft crowd, against the push-back, were content to go their own way and see just how far they could actually push their learning algorithms in solving ‘real-world’ problems (eventually using Bayesian networks).  Most early cognitive scientists of this time—while now at least partially acknowledging and accepting the fact that what they were doing was indeed not real ‘human-intelligence’ modeling—would nonetheless remain undeterred from learning about how to improve upon these non-human-like networks. One AI-soft champion that stands out here would be Frank Rosenblatt and his Perceptron model for visual learning (1959-1962).<table cellspacing="15" valign="top"><tr><td>Format: </td><td>[ <a href="/lingbuzz/006410/current.pdf">pdf</a> ]</td></tr><tr><td>Reference: </td><td>lingbuzz/006410<br/><font size="-1"> (please use that when you cite this article)</font></td></tr><tr><td>Published in: </td><td>LINCOM Studies in theoretical Linguistics, 61</td></tr><tr><td>keywords: </td><td>ai recursive implementation, language acquisition, connectionism vs symbolic rules, syntax</td></tr><tr><td>Downloaded:</td><td>380 times</td></tr></table><table cellspacing="15"><tr><p></p>&nbsp;<p></p>[ <a href="/lingbuzz/006410">edit this article</a> | <a href="/lingbuzz/006410">back to article list</a> ]</tr></table><script type="text/javascript">/*<![CDATA[*/function onLoad(){};/*]]>*/</script></body></html>