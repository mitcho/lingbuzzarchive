<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><title>Large Language Models and the Argument From the Poverty of the Stimulus - lingbuzz/006829</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8"/><meta http-equiv="Content-Script-Type" content="text/javascript"/><link rel="canonical" href="/lingbuzz/006829"/><meta name="description" content="How much of our linguistic knowledge is innate? According to much of theoretical linguistics, a fair amount. One of the best-known (and most contested) kinds of evidence for a large innate endowment i - lingbuzz, the linguistics archive"/><link rel="stylesheet" type="text/css" href="/buzzdocs/styles/article-editor.css"/><link rel="stylesheet" type="text/css" href="/lingbuzz"/></head><body alink="#111111" vlink="#333344" link="#3333AA" onload="onLoad()">&nbsp;<p></p><center><font size="+1"><b><a href="/lingbuzz/006829/current.pdf">Large Language Models and the Argument From the Poverty of the Stimulus</a></b></font><br/><a href="/lingbuzz/006829">Nur Lan</a>, <a href="/lingbuzz/006829">Emmanuel Chemla</a>, <a href="/lingbuzz/006829">Roni Katzir</a><br/>November 2023</center>&nbsp;<p></p>How much of our linguistic knowledge is innate? According to much of theoretical linguistics, a fair amount. One of the best-known (and most contested) kinds of evidence for a large innate endowment is the so-called argument from the poverty of the stimulus (APS). In a nutshell, an APS obtains when human learners systematically make inductive leaps that are not warranted by the linguistic evidence. A weakness of the APS has been that it is very hard to assess what is warranted by the linguistic evidence. Current Artificial Neural Networks appear to offer a handle on this challenge. Wilcox et al. (2021) use such models to examine the available evidence as it pertains to wh-movement. They conclude that the (presumably linguistically neutral) networks acquire an adequate knowledge of wh-movement, thus undermining an APS in this domain. We examine the evidence further and show that the networks do not, in fact, succeed in acquiring wh-movement. More tentatively, our findings suggest that the failure of the networks is due to the insufficient richness of the linguistic input and not to inadequacies of the networks, thus supporting an APS, the first that is based on successful learners exposed to realistic amounts of linguistic input.<table cellspacing="15" valign="top"><tr><td>Format: </td><td>[ <a href="/lingbuzz/006829/current.pdf">pdf</a> ]</td></tr><tr><td>Reference: </td><td>lingbuzz/006829<br/><font size="-1"> (please use that when you cite this article)</font></td></tr><tr><td>Published in: </td><td></td></tr><tr><td>keywords: </td><td>neural networks, deep learning, filler-gap dependency, syntactic islands, learnability, across-the-board movement, parasitic gaps, subject-aux inversion, language models, syntax</td></tr><tr><td>previous versions: </td><td><a href="/lingbuzz/006829/v1.pdf">v1 [September 2022]</a><br/></td></tr><tr><td>Downloaded:</td><td>1055 times</td></tr></table><table cellspacing="15"><tr><p></p>&nbsp;<p></p>[ <a href="/lingbuzz/006829">edit this article</a> | <a href="/lingbuzz/006829">back to article list</a> ]</tr></table><script type="text/javascript">/*<![CDATA[*/function onLoad(){};/*]]>*/</script></body></html>