<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><title>Syntactic Learning from Ambiguous Evidence: Errors and End-States [Dissertation] - lingbuzz/002625</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8"/><meta http-equiv="Content-Script-Type" content="text/javascript"/><link rel="canonical" href="/lingbuzz/002625"/><meta name="description" content="In this thesis I explore the role of ambiguous evidence in first language acquisition by using a probabilistic learner for setting syntactic parameters. As ambiguous evidence is input to the learner t - lingbuzz, the linguistics archive"/><link rel="stylesheet" type="text/css" href="/buzzdocs/styles/article-editor.css"/><link rel="stylesheet" type="text/css" href="/lingbuzz"/></head><body alink="#111111" vlink="#333344" link="#3333AA" onload="onLoad()">&nbsp;<p></p><center><font size="+1"><b><a href="/lingbuzz/002625/current.pdf">Syntactic Learning from Ambiguous Evidence: Errors and End-States [Dissertation]</a></b></font><br/><a href="/lingbuzz/002625">Isaac Gould</a><br/>June 2015</center>&nbsp;<p></p>In this thesis I explore the role of ambiguous evidence in first language acquisition by using a probabilistic learner for setting syntactic parameters. As ambiguous evidence is input to the learner that is compatible with multiple grammars or hypotheses, it poses learnability and acquisition challenges because it underdetermines the correct analysis. However, a probabilistic learning model with competing hypotheses can address these challenges by learning from general tendencies regarding the shape of the input, thereby finding the most compatible set of hypotheses, or the grammar with the ‘best fit’ to the input. This enables the model to resolve the challenge of learning the grammar of a subset language: it can reach such a target end-state by learning from implicit negative evidence. Moreover, ambiguous evidence can provide insight into two phenomena characteristic of language acquisition: variability (both within speakers and across a population) and learning errors. Both phenomena can be accounted for under a model that is attempting to learn a grammar of best fit.


Three case studies relating to word order and phrase structure are investigated with simulations of the model. First I show how the model can account for embedded clause verb placement errors in child Swiss German by learning from ambiguous input. I then show how learning from ambiguous input allows the model to account for grammatical variability across speakers with regard to verb movement in Korean. Finally, I show that the model is successfully able to learn the grammar of a subset language with the example of zero-derived causatives in English.<table cellspacing="15" valign="top"><tr><td>Format: </td><td>[ <a href="/lingbuzz/002625/current.pdf">pdf</a> ]</td></tr><tr><td>Reference: </td><td>lingbuzz/002625<br/><font size="-1"> (please use that when you cite this article)</font></td></tr><tr><td>keywords: </td><td>language acquisition, parameter setting, child errors, grammatical variability, subset language, verb movement, head-complement order, v2, swiss german, korean, zero-derived causatives, syntax</td></tr><tr><td>Downloaded:</td><td>2074 times</td></tr></table><table cellspacing="15"><tr><p></p>&nbsp;<p></p>[ <a href="/lingbuzz/002625">edit this article</a> | <a href="/lingbuzz/002625">back to article list</a> ]</tr></table><script type="text/javascript">/*<![CDATA[*/function onLoad(){};/*]]>*/</script></body></html>
