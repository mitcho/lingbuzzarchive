<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><title>Natural Language Inference with Mixed Effects - lingbuzz/005503</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8"/><meta http-equiv="Content-Script-Type" content="text/javascript"/><link rel="canonical" href="/lingbuzz/005503"/><meta name="description" content="There is growing evidence that the prevalence of disagreement in the raw annotations used to construct natural language inference datasets makes the common practice of aggregating those annotations to - lingbuzz, the linguistics archive"/><link rel="stylesheet" type="text/css" href="/buzzdocs/styles/article-editor.css"/><link rel="stylesheet" type="text/css" href="/lingbuzz"/></head><body alink="#111111" vlink="#333344" link="#3333AA" onload="onLoad()">&nbsp;<p></p><center><font size="+1"><b><a href="/lingbuzz/005503/current.pdf">Natural Language Inference with Mixed Effects</a></b></font><br/><a href="/lingbuzz/005503">William Gantt</a>, <a href="/lingbuzz/005503">Benjamin Kane</a>, <a href="/lingbuzz/005503">Aaron Steven White</a><br/>October 2020</center>&nbsp;<p></p>There is growing evidence that the prevalence of disagreement in the raw annotations used to construct natural language inference datasets makes the common practice of aggregating those annotations to a single label problematic. We propose a generic method that allows one to skip the aggregation step and train on the raw annotations directly without subjecting the model to unwanted noise that can arise from annotator response biases. We demonstrate that this method, which generalizes the notion of a mixed effects model by incorporating annotator random effects into any existing neural model, improves performance over models that do not incorporate such effects.  <table cellspacing="15" valign="top"><tr><td>Format: </td><td>[ <a href="/lingbuzz/005503/current.pdf">pdf</a> ]</td></tr><tr><td>Reference: </td><td>lingbuzz/005503<br/><font size="-1"> (please use that when you cite this article)</font></td></tr><tr><td>Published in: </td><td>Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM 2020)</td></tr><tr><td>keywords: </td><td>computational semantics, natural language inference, mixed effects models, deep learning, semantics</td></tr><tr><td>Downloaded:</td><td>415 times</td></tr></table><table cellspacing="15"><tr><p></p>&nbsp;<p></p>[ <a href="/lingbuzz/005503">edit this article</a> | <a href="/lingbuzz/005503">back to article list</a> ]</tr></table><script type="text/javascript">/*<![CDATA[*/function onLoad(){};/*]]>*/</script></body></html>