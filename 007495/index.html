<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><title>Characterizing English Preposing in PP constructions - lingbuzz/007495</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8"/><meta http-equiv="Content-Script-Type" content="text/javascript"/><link rel="canonical" href="/lingbuzz/007495"/><meta name="description" content="The English Preposing in PP construction (PiPP; e.g. &quot;Happy though/as we were&quot;) is extremely rare but displays an intricate set of stable syntactic properties. How do people become proficient with thi - lingbuzz, the linguistics archive"/><link rel="stylesheet" type="text/css" href="/buzzdocs/styles/article-editor.css"/><link rel="stylesheet" type="text/css" href="/lingbuzz"/></head><body alink="#111111" vlink="#333344" link="#3333AA" onload="onLoad()">&nbsp;<p></p><center><font size="+1"><b><a href="/lingbuzz/007495/current.pdf">Characterizing English Preposing in PP constructions</a></b></font><br/><a href="/lingbuzz/007495">Christopher Potts</a><br/>September 2023</center>&nbsp;<p></p>The English Preposing in PP construction (PiPP; e.g. "Happy though/as we were") is extremely rare but displays an intricate set of stable syntactic properties. How do people become proficient with this construction despite such limited evidence? It is tempting to posit innate learning mechanisms, but present-day large language models seem to learn to represent PiPPs as well, even though such models employ only very general learning mechanisms and experience very few instances of the construction during training. This suggests an alternative hypothesis on which knowledge of more frequent constructions helps shape knowledge of PiPPs. I seek to make this idea precise using model-theoretic syntax (MTS). In MTS, a grammar is essentially a set of constraints on forms. In this context, PiPPs can be seen as arising from a mix of construction-specific and general-purpose constraints, all of which seem inferable from<table cellspacing="15" valign="top"><tr><td>Format: </td><td>[ <a href="/lingbuzz/007495/current.pdf">pdf</a> ]</td></tr><tr><td>Reference: </td><td>lingbuzz/007495<br/><font size="-1"> (please use that when you cite this article)</font></td></tr><tr><td>Published in: </td><td></td></tr><tr><td>keywords: </td><td>unbounded dependency constructions, large language models, corpus   linguistics, model-theoretic syntax, stimulus poverty arguments, syntax</td></tr><tr><td>previous versions: </td><td><a href="/lingbuzz/007495/v1.pdf">v1 [August 2023]</a><br/></td></tr><tr><td>Downloaded:</td><td>633 times</td></tr></table><table cellspacing="15"><tr><p></p>&nbsp;<p></p>[ <a href="/lingbuzz/007495">edit this article</a> | <a href="/lingbuzz/007495">back to article list</a> ]</tr></table><script type="text/javascript">/*<![CDATA[*/function onLoad(){};/*]]>*/</script></body></html>