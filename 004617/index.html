<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><title>Generative Adversarial Phonology: Modeling unsupervised allophonic learning with neural networks - lingbuzz/004617</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8"/><meta http-equiv="Content-Script-Type" content="text/javascript"/><link rel="canonical" href="/lingbuzz/004617"/><meta name="description" content="This paper argues that phonetic and phonological learning can be modeled as a dependency between random space and generated speech data in the Generative Adversarial Network architecture and proposes  - lingbuzz, the linguistics archive"/><link rel="stylesheet" type="text/css" href="/buzzdocs/styles/article-editor.css"/><link rel="stylesheet" type="text/css" href="/lingbuzz"/></head><body alink="#111111" vlink="#333344" link="#3333AA" onload="onLoad()">&nbsp;<p></p><center><font size="+1"><b><a href="/lingbuzz/004617/current.pdf">Generative Adversarial Phonology: Modeling unsupervised allophonic learning with neural networks</a></b></font><br/><a href="/lingbuzz/004617">Gasper Begus</a><br/>October 2019</center>&nbsp;<p></p>This paper argues that phonetic and phonological learning can be modeled as a dependency between random space and generated speech data in the Generative Adversarial Network architecture and proposes a methodology to uncover the network’s internal representations that correspond to phonetic and phonological features. The Generative Adversarial architecture is uniquely appropriate for modeling phonetic and phonological learning because the network is trained on unannotated raw acoustic data and learning is unsupervised without any language-specific assumptions or pre-assumed levels of abstraction. A Generative Adversarial Network (Goodfellow et al. 2014, implemented for acoustic data as WaveGAN in Donahue et al. 2019) was trained on an allophonic distribution in English, in which voiceless stops surface as aspirated word-initially before stressed vowels, except if preceded by a sibilant [s]. The network successfully learns the allophonic alternation: the network’s generated speech signal contains the conditional distribution of aspiration duration. Additionally, the network generates innovative outputs for which no evidence is available in the training data, suggesting that the network segments continuous speech signal into units that can be productively recombined. The paper proposes a technique for establishing the network’s internal representations that identifies latent variables that correspond to, for example, presence of [s] and its spectral properties. By manipulating these variables, we actively control the presence of [s] and its frication amplitude in the generated outputs. This suggests that the network learns to use latent variables as an approximation of phonetic and phonological features. Crucially, we observe that the dependencies learned in training extend beyond the training interval, which allows for additional exploration of learning representations. The paper also discusses how the network’s architecture and innovative outputs resemble and differ from linguistic behavior in language acquisition, speech disorders, and speech errors.<table cellspacing="15" valign="top"><tr><td>Format: </td><td>[ <a href="/lingbuzz/004617/current.pdf">pdf</a> ]</td></tr><tr><td>Reference: </td><td>lingbuzz/004617<br/><font size="-1"> (please use that when you cite this article)</font></td></tr><tr><td>Published in: </td><td>Submitted.</td></tr><tr><td>keywords: </td><td>artificial intelligence, neural networks, generative adversarial networks, phonetic learning, phonological learning, voice onset time, allophonic distribution, phonology</td></tr><tr><td>previous versions: </td><td><a href="/lingbuzz/004617/v5.pdf">v5 [October 2019]</a><br/><a href="/lingbuzz/004617/v4.pdf">v4 [October 2019]</a><br/><a href="/lingbuzz/004617/v3.pdf">v3 [August 2019]</a><br/><a href="/lingbuzz/004617/v2.pdf">v2 [July 2019]</a><br/><a href="/lingbuzz/004617/v1.pdf">v1 [May 2019]</a><br/></td></tr><tr><td>Downloaded:</td><td>272 times</td></tr></table><table cellspacing="15"><tr><p></p>&nbsp;<p></p>[ <a href="/lingbuzz/004617">edit this article</a> | <a href="/lingbuzz/004617">back to article list</a> ]</tr></table><script type="text/javascript">/*<![CDATA[*/function onLoad(){};/*]]>*/</script></body></html>
