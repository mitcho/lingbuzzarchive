<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><title>Co-linguistic content projection: From gestures to sound effects and emoji - lingbuzz/005082</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8"/><meta http-equiv="Content-Script-Type" content="text/javascript"/><link rel="canonical" href="/lingbuzz/005082"/><meta name="description" content="NEW TITLE: Co-linguistic content inferences: From gestures to sound effects and emoji&lt;/br&gt;&lt;/br&gt;

Among other uses, co-speech gestures can contribute additional semantic content to the spoken utteran - lingbuzz, the linguistics archive"/><link rel="stylesheet" type="text/css" href="/buzzdocs/styles/article-editor.css"/><link rel="stylesheet" type="text/css" href="/lingbuzz"/></head><body alink="#111111" vlink="#333344" link="#3333AA" onload="onLoad()">&nbsp;<p></p><center><font size="+1"><b><a href="/lingbuzz/005082/current.pdf">Co-linguistic content projection: From gestures to sound effects and emoji</a></b></font><br/><a href="/lingbuzz/005082">Robert Pasternak</a>, <a href="/lingbuzz/005082">Lyn Tieu</a><br/>April 2022</center>&nbsp;<p></p>NEW TITLE: Co-linguistic content inferences: From gestures to sound effects and emoji</br></br>

Among other uses, co-speech gestures can contribute additional semantic content to the spoken utterances with which they coincide. A growing body of research is dedicated to understanding how inferences from gestures interact with logical operators in speech, including negation ("not"/"n't"), modals (e.g., "might"), and quantifiers (e.g., "each", "none", "exactly one"). A related but less-addressed question is what kinds of meaningful content other than gestures can evince this same behavior; this is in turn connected to the much broader question of what properties of gestures are responsible for how they interact with logical operators. We present two experiments investigating sentences with co-speech sound effects and co-text emoji in lieu of gestures, revealing a remarkably similar inference pattern to that of co-speech gestures. The results suggest that gestural inferences do not behave the way they do because of any traits specific to gestures, and that the inference pattern extends to a much broader range of content.<table cellspacing="15" valign="top"><tr><td>Format: </td><td>[ <a href="/lingbuzz/005082/current.pdf">pdf</a> ]</td></tr><tr><td>Reference: </td><td>lingbuzz/005082<br/><font size="-1"> (please use that when you cite this article)</font></td></tr><tr><td>Published in: </td><td>Quarterly Journal of Experimental Psychology</td></tr><tr><td>keywords: </td><td>co-linguistic content, gesture, emoji, sound effects, semantics</td></tr><tr><td>previous versions: </td><td><a href="/lingbuzz/005082/v3.pdf">v3 [October 2021]</a><br/><a href="/lingbuzz/005082/v2.pdf">v2 [May 2021]</a><br/><a href="/lingbuzz/005082/v1.pdf">v1 [March 2020]</a><br/></td></tr><tr><td>Downloaded:</td><td>1252 times</td></tr></table><table cellspacing="15"><tr><p></p>&nbsp;<p></p>[ <a href="/lingbuzz/005082">edit this article</a> | <a href="/lingbuzz/005082">back to article list</a> ]</tr></table><script type="text/javascript">/*<![CDATA[*/function onLoad(){};/*]]>*/</script></body></html>