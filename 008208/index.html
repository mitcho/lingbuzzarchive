<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><title>Generating event descriptions under syntactic and semantic constraints - lingbuzz/008208</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8"/><meta http-equiv="Content-Script-Type" content="text/javascript"/><link rel="canonical" href="/lingbuzz/008208"/><meta name="description" content="With the goal of supporting scalable lexical semantic annotation, analysis, and theorizing, we conduct a comprehensive evaluation of different methods for generating event descriptions under both synt - lingbuzz, the linguistics archive"/><link rel="stylesheet" type="text/css" href="/buzzdocs/styles/article-editor.css"/><link rel="stylesheet" type="text/css" href="/lingbuzz"/></head><body alink="#111111" vlink="#333344" link="#3333AA" onload="onLoad()">&nbsp;<p></p><center><font size="+1"><b><a href="/lingbuzz/008208/current.pdf">Generating event descriptions under syntactic and semantic constraints</a></b></font><br/><a href="/lingbuzz/008208">Angela Cao</a>, <a href="/lingbuzz/008208">Faye Holt</a>, <a href="/lingbuzz/008208">Jonas Chan</a>, <a href="/lingbuzz/008208">Stephanie Richter</a>, <a href="/lingbuzz/008208">Lelia Glass</a>, <a href="/lingbuzz/008208">Aaron Steven White</a><br/>June 2024</center>&nbsp;<p></p>With the goal of supporting scalable lexical semantic annotation, analysis, and theorizing, we conduct a comprehensive evaluation of different methods for generating event descriptions under both syntactic constraints–e.g. desired clause structure–and semantic constraints–e.g. desired verb sense. We compare three different methods–(i) manual generation by experts; (ii) sampling from a corpus annotated for syntactic and semantic information; and (iii) sampling from a language model (LM) conditioned on syntactic and semantic information–along three dimensions of the generated event descriptions: (a) naturalness, (b) typicality, and (c) distinctiveness. We find that all methods reliably produce natural, typical, and distinctive event descriptions, but that manual generation continues to produce event descriptions that are more natural, typical, and distinctive than the automated generation methods. We conclude that the automated methods we consider produce event descriptions of sufficient quality for use in downstream annotation and analysis insofar as the methods used for this annotation and analysis are robust to a small amount of degradation in the resulting event descriptions.<table cellspacing="15" valign="top"><tr><td>Format: </td><td>[ <a href="/lingbuzz/008208/current.pdf">pdf</a> ]</td></tr><tr><td>Reference: </td><td>lingbuzz/008208<br/><font size="-1"> (please use that when you cite this article)</font></td></tr><tr><td>Published in: </td><td></td></tr><tr><td>keywords: </td><td>lexical semantics, corpus, language model, experiment, naturalness, typicality, distinctiveness, semantics, syntax</td></tr><tr><td>Downloaded:</td><td>331 times</td></tr></table><table cellspacing="15"><tr><p></p>&nbsp;<p></p>[ <a href="/lingbuzz/008208">edit this article</a> | <a href="/lingbuzz/008208">back to article list</a> ]</tr></table><script type="text/javascript">/*<![CDATA[*/function onLoad(){};/*]]>*/</script></body></html>