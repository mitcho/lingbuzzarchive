<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><title>Probing self-supervised speech models for phonetic and phonemic information: a case study in aspiration - lingbuzz/007340</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8"/><meta http-equiv="Content-Script-Type" content="text/javascript"/><link rel="canonical" href="/lingbuzz/007340"/><meta name="description" content="Textless self-supervised speech models have grown in capabilities in recent years, but the nature of the linguistic information they encode has not yet been thoroughly examined. We evaluate the extent - lingbuzz, the linguistics archive"/><link rel="stylesheet" type="text/css" href="/buzzdocs/styles/article-editor.css"/><link rel="stylesheet" type="text/css" href="/lingbuzz"/></head><body alink="#111111" vlink="#333344" link="#3333AA" onload="onLoad()">&nbsp;<p></p><center><font size="+1"><b><a href="/lingbuzz/007340/current.pdf">Probing self-supervised speech models for phonetic and phonemic information: a case study in aspiration</a></b></font><br/><a href="/lingbuzz/007340">Kinan Martin</a>, <a href="/lingbuzz/007340">Jon Gauthier</a>, <a href="/lingbuzz/007340">Canaan Breiss</a>, <a href="/lingbuzz/007340">Roger Levy</a><br/>June 2023</center>&nbsp;<p></p>Textless self-supervised speech models have grown in capabilities in recent years, but the nature of the linguistic information they encode has not yet been thoroughly examined. We evaluate the extent to which these models' learned representations align with basic representational distinctions made by humans, focusing on a set of phonetic (low-level) and phonemic (more abstract) contrasts instantiated in word-initial stops. We find that robust representations of both phonetic and phonemic distinctions emerge in early layers of these models' architectures, and are preserved in the principal components of deeper layer representations. Our analyses suggest two sources for this success: some can only be explained by the optimization of the models on speech data, while some can be attributed to these models' high-dimensional architectures.
Our findings show that speech-trained HuBERT derives a low-noise and low-dimensional subspace corresponding to abstract phonological distinctions.<table cellspacing="15" valign="top"><tr><td>Format: </td><td>[ <a href="/lingbuzz/007340/current.pdf">pdf</a> ]</td></tr><tr><td>Reference: </td><td>lingbuzz/007340<br/><font size="-1"> (please use that when you cite this article)</font></td></tr><tr><td>Published in: </td><td>Proceedings of INTERSPEECH 2023</td></tr><tr><td>keywords: </td><td>self-supervised models, decoding analysis, probing, speech, representation learning, phonemes, phonology</td></tr><tr><td>Downloaded:</td><td>436 times</td></tr></table><table cellspacing="15"><tr><p></p>&nbsp;<p></p>[ <a href="/lingbuzz/007340">edit this article</a> | <a href="/lingbuzz/007340">back to article list</a> ]</tr></table><script type="text/javascript">/*<![CDATA[*/function onLoad(){};/*]]>*/</script></body></html>