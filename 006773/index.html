<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><title>What Artificial Neural Networks Can Tell Us About Human Language Acquisition - lingbuzz/006773</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8"/><meta http-equiv="Content-Script-Type" content="text/javascript"/><link rel="canonical" href="/lingbuzz/006773"/><meta name="description" content="Rapid progress in machine learning for natural language processing has the potential to transform debates about how humans learn language. However, the learning environments and biases of current arti - lingbuzz, the linguistics archive"/><link rel="stylesheet" type="text/css" href="/buzzdocs/styles/article-editor.css"/><link rel="stylesheet" type="text/css" href="/lingbuzz"/></head><body alink="#111111" vlink="#333344" link="#3333AA" onload="onLoad()">&nbsp;<p></p><center><font size="+1"><b><a href="/lingbuzz/006773/current.pdf">What Artificial Neural Networks Can Tell Us About Human Language Acquisition</a></b></font><br/><a href="/lingbuzz/006773">Alex Warstadt</a>, <a href="/lingbuzz/006773">Samuel Bowman</a><br/>August 2022</center>&nbsp;<p></p>Rapid progress in machine learning for natural language processing has the potential to transform debates about how humans learn language. However, the learning environments and biases of current artificial learners and humans diverge in ways that weaken the impact of the evidence obtained from learning simulations. For example, today's most effective neural language models are trained on roughly one thousand times the amount of linguistic data available to a typical child. To increase the relevance of learnability results from computational models, we need to train model learners without significant advantages over humans. 
If an appropriate model successfully acquires some target linguistic knowledge, it can provide a proof of concept that the target is learnable in a hypothesized human learning scenario.
Plausible model learners will enable us to carry out experimental manipulations to make causal inferences about variables in the learning environment, and to rigorously test poverty-of-the-stimulus-style claims arguing for innate linguistic knowledge in humans on the basis of speculations about learnability. Comparable experiments will never be possible with human subjects due to practical and ethical considerations, making model learners an indispensable resource.
So far, attempts to deprive current models of unfair advantages obtain sub-human results for key grammatical behaviors such as acceptability judgments. But before we can justifiably conclude that language learning requires more prior domain-specific knowledge than current models possess, we must first explore non-linguistic inputs in the form of multimodal stimuli and multi-agent interaction as ways to make our learners more efficient at learning from limited linguistic input. <table cellspacing="15" valign="top"><tr><td>Format: </td><td>[ <a href="/lingbuzz/006773/current.pdf">pdf</a> ]</td></tr><tr><td>Reference: </td><td>lingbuzz/006773<br/><font size="-1"> (please use that when you cite this article)</font></td></tr><tr><td>Published in: </td><td>forthcoming in Algebraic Structures in Natural Language. Shalom Lappin & Jean-Philippe Bernardy, editors. Taylor & Francis.</td></tr><tr><td>keywords: </td><td>acquisition, machine learning, nlp, cognitive modeling, syntax, phonology, semantics, morphology</td></tr><tr><td>previous versions: </td><td><a href="/lingbuzz/006773/v1.pdf">v1 [August 2022]</a><br/></td></tr><tr><td>Downloaded:</td><td>725 times</td></tr></table><table cellspacing="15"><tr><p></p>&nbsp;<p></p>[ <a href="/lingbuzz/006773">edit this article</a> | <a href="/lingbuzz/006773">back to article list</a> ]</tr></table><script type="text/javascript">/*<![CDATA[*/function onLoad(){};/*]]>*/</script></body></html>